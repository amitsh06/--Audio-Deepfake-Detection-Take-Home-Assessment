{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Deepfake Detection: LCNN with Attention Mechanism\n",
    "\n",
    "This notebook implements an audio deepfake detection system using a Light Convolutional Neural Network (LCNN) with Attention Mechanism. The implementation is based on research from the [Audio-Deepfake-Detection](https://github.com/media-sec-lab/Audio-Deepfake-Detection) repository.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The LCNN with Attention Mechanism was selected for implementation due to its excellent balance between performance and computational efficiency. The attention mechanism is particularly well-suited for detecting the subtle artifacts present in deepfake audio, and the lightweight architecture makes it feasible for near real-time applications.\n",
    "\n",
    "This notebook covers:\n",
    "1. Dataset download and preparation\n",
    "2. Audio preprocessing and feature extraction\n",
    "3. LCNN with Attention model implementation\n",
    "4. Model training and evaluation\n",
    "5. Performance analysis and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "# Audio processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Scikit-learn for evaluation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Download and Preparation\n",
    "\n",
    "For this implementation, we'll use the ASVspoof 2019 Logical Access (LA) dataset, which is widely used for audio deepfake detection research. The dataset contains both genuine and spoofed audio samples.\n",
    "\n",
    "### 2.1 Download Instructions\n",
    "\n",
    "The ASVspoof 2019 dataset can be downloaded from the official website: [ASVspoof 2019](https://www.asvspoof.org/index2019.html).\n",
    "\n",
    "For this implementation, we'll use a small subset of the dataset for demonstration purposes. In a real-world scenario, you would download the full dataset and extract it to the `data/ASVspoof2019/` directory.\n",
    "\n",
    "### 2.2 Dataset Structure\n",
    "\n",
    "The ASVspoof 2019 LA dataset is organized as follows:\n",
    "\n",
    "```\n",
    "data/\n",
    "├── ASVspoof2019/\n",
    "│   ├── LA/\n",
    "│   │   ├── ASVspoof2019_LA_train/\n",
    "│   │   ├── ASVspoof2019_LA_dev/\n",
    "│   │   └── ASVspoof2019_LA_eval/\n",
    "│   └── protocols/\n",
    "```\n",
    "\n",
    "### 2.3 Creating a Small Demo Dataset\n",
    "\n",
    "Since downloading the full dataset might be time-consuming, we'll create a small demo dataset for this implementation. In a real-world scenario, you would use the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for the demo dataset\n",
    "base_dir = '../data/ASVspoof2019_demo/'\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(base_dir, 'genuine'), exist_ok=True)\n",
    "os.makedirs(os.path.join(base_dir, 'spoof'), exist_ok=True)\n",
    "\n",
    "# Function to generate synthetic audio for demonstration purposes\n",
    "def generate_demo_audio(duration=3, sr=16000, is_genuine=True):\n",
    "    \"\"\"\n",
    "    Generate synthetic audio for demonstration purposes.\n",
    "    Parameters:\n",
    "    - duration: Duration of the audio in seconds\n",
    "    - sr: Sampling rate\n",
    "    - is_genuine: Whether to generate genuine or spoofed audio\n",
    "    \n",
    "    Returns:\n",
    "    - audio: Synthetic audio signal\n",
    "    \"\"\"\n",
    "    # For genuine audio, we'll use a clean sine wave\n",
    "    if is_genuine:\n",
    "        t = np.linspace(0, duration, int(sr * duration), endpoint=False)\n",
    "        # Generate a clean sine wave with some harmonics\n",
    "        audio = 0.5 * np.sin(2 * np.pi * 440 * t) + 0.3 * np.sin(2 * np.pi * 880 * t) + 0.2 * np.sin(2 * np.pi * 1320 * t)\n",
    "        # Add a small amount of noise\n",
    "        audio += 0.05 * np.random.randn(len(audio))\n",
    "    # For spoofed audio, we'll add artifacts that mimic deepfake characteristics\n",
    "    else:\n",
    "        t = np.linspace(0, duration, int(sr * duration), endpoint=False)\n",
    "        # Generate a sine wave with some harmonics\n",
    "        audio = 0.5 * np.sin(2 * np.pi * 440 * t) + 0.3 * np.sin(2 * np.pi * 880 * t) + 0.2 * np.sin(2 * np.pi * 1320 * t)\n",
    "        # Add more noise to simulate artifacts\n",
    "        audio += 0.2 * np.random.randn(len(audio))\n",
    "        # Add some frequency artifacts that are common in deepfakes\n",
    "        artifact = 0.1 * np.sin(2 * np.pi * 4000 * t)\n",
    "        audio += artifact\n",
    "        # Add some temporal discontinuities\n",
    "        for i in range(5):\n",
    "            idx = np.random.randint(0, len(audio) - 100)\n",
    "            audio[idx:idx+100] *= 0.7\n",
    "    \n",
    "    # Normalize the audio\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "    return audio\n",
    "\n",
    "# Generate demo dataset\n",
    "num_samples = 100  # 50 genuine, 50 spoofed\n",
    "\n",
    "print('Generating demo dataset...')\n",
    "for i in tqdm(range(num_samples)):\n",
    "    is_genuine = i < num_samples // 2\n",
    "    audio = generate_demo_audio(duration=3, sr=16000, is_genuine=is_genuine)\n",
    "    \n",
    "    # Save the audio file\n",
    "    if is_genuine:\n",
    "        filename = os.path.join(base_dir, 'genuine', f'genuine_{i:04d}.wav')\n",
    "    else:\n",
    "        filename = os.path.join(base_dir, 'spoof', f'spoof_{i-num_samples//2:04d}.wav')\n",
    "    \n",
    "    sf.write(filename, audio, 16000)\n",
    "\n",
    "# Create metadata file\n",
    "metadata = []\n",
    "for i in range(num_samples // 2):\n",
    "    metadata.append({'file': f'genuine/genuine_{i:04d}.wav', 'label': 'genuine'})\n",
    "for i in range(num_samples // 2):\n",
    "    metadata.append({'file': f'spoof/spoof_{i:04d}.wav', 'label': 'spoof'})\n",
    "\n",
    "# Convert to DataFrame and save\n",
    "metadata_df = pd.DataFrame(metadata)\n",
    "metadata_df.to_csv(os.path.join(base_dir, 'metadata.csv'), index=False)\n",
    "\n",
    "print(f'Demo dataset created with {num_samples} samples ({num_samples//2} genuine, {num_samples//2} spoofed)')\n",
    "print(f'Dataset saved to {base_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Load and Explore the Dataset\n",
    "\n",
    "Now that we have our demo dataset, let's load and explore it to understand its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata\n",
    "metadata_df = pd.read_csv(os.path.join(base_dir, 'metadata.csv'))\n",
    "\n",
    "# Display the first few rows\n",
    "print('Dataset overview:')\n",
    "print(metadata_df.head())\n",
    "\n",
    "# Count the number of samples in each class\n",
    "print('\nClass distribution:')\n",
    "print(metadata_df['label'].value_counts())\n",
    "\n",
    "# Visualize a genuine and a spoofed audio sample\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Load a genuine audio sample\n",
    "genuine_file = os.path.join(base_dir, metadata_df[metadata_df['label'] == 'genuine']['file'].iloc[0])\n",
    "genuine_audio, sr = librosa.load(genuine_file, sr=None)\n",
    "\n",
    "# Load a spoofed audio sample\n",
    "spoof_file = os.path.join(base_dir, metadata_df[metadata_df['label'] == 'spoof']['file'].iloc[0])\n",
    "spoof_audio, sr = librosa.load(spoof_file, sr=None)\n",
    "\n",
    "# Plot waveforms\n",
    "plt.subplot(2, 2, 1)\n",
    "librosa.display.waveshow(genuine_audio, sr=sr)\n",
    "plt.title('Genuine Audio Waveform')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "librosa.display.waveshow(spoof_audio, sr=sr)\n",
    "plt.title('Spoofed Audio Waveform')\n",
    "\n",
    "# Plot spectrograms\n",
    "plt.subplot(2, 2, 3)\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(genuine_audio)), ref=np.max)\n",
    "librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Genuine Audio Spectrogram')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(spoof_audio)), ref=np.max)\n",
    "librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Spoofed Audio Spectrogram')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Audio Preprocessing and Feature Extraction\n",
    "\n",
    "Now, let's preprocess the audio data and extract relevant features for our model. We'll use Mel-frequency cepstral coefficients (MFCCs) as our primary features, as they are effective for capturing the spectral characteristics of audio signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature extraction function\n",
    "def extract_features(audio_path, n_mfcc=40, n_fft=1024, hop_length=512, duration=3, sr=16000):\n",
    "    \"\"\"\n",
    "    Extract MFCC features from an audio file.\n",
    "    Parameters:\n",
    "    - audio_path: Path to the audio file\n",
    "    - n_mfcc: Number of MFCC coefficients to extract\n",
    "    - n_fft: FFT window size\n",
    "    - hop_length: Hop length for the FFT window\n",
    "    - duration: Duration of audio to consider (in seconds)\n",
    "    - sr: Sampling rate\n",
    "    \n",
    "    Returns:\n",
    "    - mfccs: MFCC features\n",
    "    \"\"\"\n",
    "    # Load audio file\n",
    "    try:\n",
    "        audio, sr = librosa.load(audio_path, sr=sr)\n",
    "        \n",
    "        # Trim or pad audio to fixed length\n",
    "        target_length = int(duration * sr)\n",
    "        if len(audio) > target_length:\n",
    "            audio = audio[:target_length]\n",
    "        else:\n",
    "            audio = np.pad(audio, (0, target_length - len(audio)), 'constant')\n",
    "        \n",
    "        # Extract MFCCs\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "        \n",
    "        # Add delta and delta-delta features (first and second derivatives)\n",
    "        delta_mfccs = librosa.feature.delta(mfccs)\n",
    "        delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
    "        \n",
    "        # Stack all features\n",
    "        features = np.concatenate([mfccs, delta_mfccs, delta2_mfccs])\n",
    "        \n",
    "        # Transpose to get time as the first dimension\n",
    "        features = features.T\n",
    "        \n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f'Error extracting features from {audio_path}: {e}')\n",
    "        return None\n",
    "\n",
    "# Create a custom dataset class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, metadata_df, base_dir, transform=None):\n",
    "        self.metadata_df = metadata_df\n",
    "        self.base_dir = base_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Map labels to integers\n",
    "        self.label_map = {'genuine': 0, 'spoof': 1}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.metadata_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get file path and label\n",
    "        file_path = os.path.join(self.base_dir, self.metadata_df.iloc[idx]['file'])\n",
    "        label = self.metadata_df.iloc[idx]['label']\n",
    "        \n",
    "        # Extract features\n",
    "        features = extract_features(file_path)\n",
    "        \n",
    "        # Convert label to integer\n",
    "        label_idx = self.label_map[label]\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "        \n",
    "        return torch.FloatTensor(features), label_idx\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(metadata_df, test_size=0.2, random_state=SEED, stratify=metadata_df['label'])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AudioDataset(train_df, base_dir)\n",
    "val_dataset = AudioDataset(val_df, base_dir)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Check the shape of the features\n",
    "sample_features, sample_label = next(iter(train_loader))\n",
    "print(f'Feature shape: {sample_features.shape}')\n",
    "print(f'Label shape: {sample_label.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LCNN with Attention Model Implementation\n",
    "\n",
    "Now, let's implement the Light Convolutional Neural Network (LCNN) with Attention Mechanism. The model consists of:\n",
    "\n",
    "1. Convolutional layers with Max-Feature-Map activations\n",
    "2. Attention mechanism to focus on the most discriminative time-frequency regions\n",
    "3. Fully connected layers for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Max-Feature-Map activation\n",
    "class MaxFeatureMap(nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super(MaxFeatureMap, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Split the channels into two groups and take the maximum\n",
    "        # This reduces the number of channels by half\n",
    "        out = torch.max(x[:, :self.out_channels//2, :, :], x[:, self.out_channels//2:, :, :])\n",
    "        return out\n",
    "\n",
    "# Define the attention mechanism\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=8):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, in_channels // reduction_ratio),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_channels // reduction_ratio, in_channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "# Define the LCNN with Attention model\n",
    "class LCNNWithAttention(nn.Module):\n",
    "    def __init__(self, input_channels=120, num_classes=2):\n",
    "        super(LCNNWithAttention, self).__init__()\n",
    "        \n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            MaxFeatureMap(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Second convolutional block with attention\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            MaxFeatureMap(128),\n",
    "            AttentionModule(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            MaxFeatureMap(256),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Fourth convolutional block with attention\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            MaxFeatureMap(512),\n",
    "            AttentionModule(256),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Adaptive pooling to handle variable input sizes\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape input to add channel dimension [batch, time, features] -> [batch, 1, time, features]\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # Apply convolutional blocks\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
